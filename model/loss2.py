import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import torch.fft as fft

def regression_loss(inputs, targets):
    # regression loss

    criterion = nn.MSELoss()
    return criterion(inputs, targets)


def bdcn_loss2(inputs, targets, l_weight=1.1):
    # bdcn loss modified in DexiNed

    targets = targets.long()
    mask = targets.float()
    num_positive = torch.sum((mask > 0.0).float()).float() # >0.1
    num_negative = torch.sum((mask <= 0.0).float()).float() # <= 0.1

    mask[mask > 0.] = 1.0 * num_negative / (num_positive + num_negative) #0.1
    mask[mask <= 0.] = 1.1 * num_positive / (num_positive + num_negative)  # before mask[mask <= 0.1]
    inputs= torch.sigmoid(inputs)
    cost = torch.nn.BCELoss(mask, reduction='none')(inputs, targets.float())
    cost = torch.sum(cost.float().mean((1, 2, 3))) # before sum
    return l_weight*cost

# ------------ cats losses ----------

def bdrloss(prediction, label, radius,device='cpu'):
    '''
    The boundary tracing loss that handles the confusing pixels.
    '''

    filt = torch.ones(1, 1, 2*radius+1, 2*radius+1)
    filt.requires_grad = False
    filt = filt.to(device)

    bdr_pred = prediction * label
    pred_bdr_sum = label * F.conv2d(bdr_pred, filt, bias=None, stride=1, padding=radius)



    texture_mask = F.conv2d(label.float(), filt, bias=None, stride=1, padding=radius)
    mask = (texture_mask != 0).float()
    mask[label == 1] = 0
    pred_texture_sum = F.conv2d(prediction * (1-label) * mask, filt, bias=None, stride=1, padding=radius)

    softmax_map = torch.clamp(pred_bdr_sum / (pred_texture_sum + pred_bdr_sum + 1e-10), 1e-10, 1 - 1e-10)
    cost = -label * torch.log(softmax_map)
    cost[label == 0] = 0

    return torch.sum(cost.float().mean((1, 2, 3)))



def textureloss(prediction, label, mask_radius, device='cpu'):
    '''
    The texture suppression loss that smooths the texture regions.
    '''
    filt1 = torch.ones(1, 1, 3, 3)
    filt1.requires_grad = False
    filt1 = filt1.to(device)
    filt2 = torch.ones(1, 1, 2*mask_radius+1, 2*mask_radius+1)
    filt2.requires_grad = False
    filt2 = filt2.to(device)

    pred_sums = F.conv2d(prediction.float(), filt1, bias=None, stride=1, padding=1)
    label_sums = F.conv2d(label.float(), filt2, bias=None, stride=1, padding=mask_radius)

    mask = 1 - torch.gt(label_sums, 0).float()

    loss = -torch.log(torch.clamp(1-pred_sums/9, 1e-10, 1-1e-10))
    loss[mask == 0] = 0

    return torch.sum(loss.float().mean((1, 2, 3)))


def cats_loss(prediction, label, l_weight=[0.,0.], device='cpu'):
    # tracingLoss

    tex_factor,bdr_factor = l_weight
    balanced_w = 1.1
    label = label.float()
    prediction = prediction.float()
    with torch.no_grad():
        mask = label.clone()

        num_positive = torch.sum((mask == 1).float()).float()
        num_negative = torch.sum((mask == 0).float()).float()
        beta = num_negative / (num_positive + num_negative)
        mask[mask == 1] = beta
        mask[mask == 0] = balanced_w * (1 - beta)
        mask[mask == 2] = 0
    prediction = torch.sigmoid(prediction)

    cost = torch.nn.functional.binary_cross_entropy(
        prediction.float(), label.float(), weight=mask, reduction='none')
    cost = torch.sum(cost.float().mean((1, 2, 3)))  # by me
    label_w = (label != 0).float()
    textcost = textureloss(prediction.float(), label_w.float(), mask_radius=4, device=device)
    bdrcost = bdrloss(prediction.float(), label_w.float(), radius=4, device=device)

    return cost + bdr_factor * bdrcost + tex_factor * textcost

def Dice_loss(prediction, label, l_weight=[0], device='cpu'):
    smooth = 1e-5
    prediction = prediction.to(device).float()  # å¼ºåˆ¶è½¬ç§»åˆ° CPU
    label = label.to(device).float()  # å¼ºåˆ¶è½¬ç§»åˆ° CPU
    prediction = torch.sigmoid(prediction)
    prediction = prediction.view(-1)
    label = label.view(-1)
    intersection = (prediction * label).sum()
    union = prediction.sum() + label.sum()
    diec = ((2.0 * intersection + smooth) / (union + smooth)) * l_weight
    return 1.0 - diec


import torch
import torch.nn as nn


class RankEDLoss(nn.Module):
    def __init__(self, alpha=0.5, margin=0.1, num_samples=5000):
        super().__init__()
        self.alpha = alpha  # LSortæŸå¤±çš„æƒé‡ç³»æ•°
        self.margin = margin  # æŽ’åºé—´éš”é˜ˆå€¼
        self.num_samples = num_samples  # æœ€å¤§é‡‡æ ·å¯¹æ•°

    def forward(self, pred, target, certainty=None):
        """
        pred:     æ¨¡åž‹è¾“å‡º [N, H, W]
        target:   çœŸå®žæ ‡ç­¾ [N, H, W] (å€¼åº”ä¸º0æˆ–1)
        certainty: ç¡®å®šæ€§åˆ†æ•° [N, H, W] (å¯é€‰)
        """
        # ç±»åž‹å®‰å…¨é¢„å¤„ç†
        pred = torch.sigmoid(pred).float()
        target = target.float()

        # å±•å¹³å¼ é‡å¤„ç†
        pred_flat = pred.view(-1)  # [B*H*W]
        target_flat = target.view(-1)  # [B*H*W]
        pos_mask = target_flat == 1  # å¸ƒå°”ç±»åž‹æŽ©ç 

        # æ­£æ ·æœ¬æ•°é‡æ£€æŸ¥
        num_pos = pos_mask.sum().int().item()
        if num_pos < 2:
            return torch.tensor(0.0, device=pred.device)

        # ================= å…¨å±€æŽ’åæŸå¤± LRank =================
        # ç”Ÿæˆæµ®ç‚¹ç±»åž‹çš„æŽ’åå¼ é‡
        _, indices = torch.sort(pred_flat, descending=True)
        ranks = torch.arange(
            1, len(pred_flat) + 1,
            dtype=torch.float32,
            device=pred.device
        )[indices]

        # æå–æ­£æ ·æœ¬æŽ’åå¹¶è®¡ç®—å‡å€¼
        l_rank = ranks[pos_mask].mean() / len(pred_flat)  # å½’ä¸€åŒ–åˆ°[0,1]

        # ================= ä¼˜åŒ–åŽçš„æŽ’åºæŸå¤± LSort =================
        # åŠ¨æ€è°ƒæ•´é‡‡æ ·æ•°é‡
        max_pairs = num_pos * (num_pos - 1)
        m = min(self.num_samples, max_pairs)

        # ç”Ÿæˆéšæœºæ ·æœ¬å¯¹ç´¢å¼•
        indices = torch.randint(
            0, num_pos,
            size=(2, m),
            device=pred.device
        )

        # è¿‡æ»¤i==jçš„æ— æ•ˆå¯¹
        valid_mask = indices[0] != indices[1]
        i = indices[0][valid_mask]
        j = indices[1][valid_mask]

        # è®¡ç®—é¢„æµ‹å·®å¼‚
        pos_scores = pred_flat[pos_mask]  # [num_pos]
        pred_diff = pos_scores[i] - pos_scores[j]
        valid_pairs = (pred_diff < self.margin).float()  # [valid_pairs]

        # å¤„ç†ç¡®å®šæ€§åˆ†æ•°
        if certainty is not None:
            pos_certainty = certainty.view(-1)[pos_mask]  # [num_pos]
            c_diff = (pos_certainty[i] - pos_certainty[j] + 1) / 2  # æ˜ å°„åˆ°[0,1]
            loss_terms = valid_pairs * (1 - c_diff)
        else:
            loss_terms = valid_pairs

        # è®¡ç®—å‡å€¼æŸå¤±ï¼ˆå¤„ç†æ— æœ‰æ•ˆé‡‡æ ·æƒ…å†µï¼‰
        l_sort = loss_terms.mean() if len(loss_terms) > 0 else 0.0

        return l_rank + self.alpha * l_sort


class FocalLoss(nn.Module):
    def __init__(self, alpha=0.8, gamma=2.0, reduction='mean'):
        """
        Focal Loss äºŒåˆ†ç±»å®žçŽ°
        :param alpha: æ­£æ ·æœ¬æƒé‡ (ç”¨äºŽç±»åˆ«å¹³è¡¡ï¼Œå»ºè®®0.75-0.95)
        :param gamma: å›°éš¾æ ·æœ¬è°ƒèŠ‚å› å­ (Î³â†‘ æ›´å…³æ³¨å›°éš¾æ ·æœ¬)
        :param reduction: æŸå¤±èšåˆæ–¹å¼ ('mean', 'sum', 'none')
        """
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        # è¾“å…¥æ ¡éªŒ
        targets = targets.squeeze(1)  # å…³é”®ä¿®æ”¹
        assert inputs.shape == targets.shape, \
            f"é¢„æµ‹å€¼ä¸Žæ ‡ç­¾å½¢çŠ¶ä¸åŒ¹é…: inputs {inputs.shape}, targets {targets.shape}"

        # è®¡ç®—äºŒåˆ†ç±»äº¤å‰ç†µ (æ— éœ€sigmoidï¼Œä½¿ç”¨logitsæ›´ç¨³å®š)
        bce_loss = F.binary_cross_entropy_with_logits(
            inputs, targets, reduction='none'
        )

        # è®¡ç®—æ¦‚çŽ‡å€¼pt
        pt = torch.exp(-bce_loss)  # pt = p if y=1 else 1-p
        focal_term = (1 - pt) ** self.gamma

        # åº”ç”¨ç±»åˆ«æƒé‡alpha
        alpha_factor = self.alpha * targets + (1 - self.alpha) * (1 - targets)

        # ç»„åˆå¾—åˆ°Focal Loss
        loss = alpha_factor * focal_term * bce_loss

        # èšåˆæ–¹å¼
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss

class HybridLoss(nn.Module):
    def __init__(self,
                 max_epochs=50,
                 scheduler_type='cosine',
                 hard_threshold=(0.3, 0.7),
                 hard_weight=2.0,
                 hard_gamma=2.0,):
        super().__init__()
        # åŸºç¡€æŸå¤±ç»„ä»¶
        self.rank_loss = RankEDLoss(alpha=0,num_samples=10000)
        self.focal_loss = FocalLoss(gamma=hard_gamma)

        # æ¸è¿›å¼è°ƒåº¦å‚æ•°
        self.max_epochs = max_epochs
        self.scheduler_type = scheduler_type
        self.current_epoch = 0

        # å›°éš¾æ ·æœ¬å‚æ•°
        self.hard_threshold = hard_threshold
        self.hard_weight = hard_weight
        self.hard_gamma = hard_gamma

    def _get_weight_ratio(self):
        """æ ¹æ®è°ƒåº¦ç±»åž‹è®¡ç®—å½“å‰æƒé‡æ¯”ä¾‹"""
        if self.scheduler_type == 'cosine':
            ratio = 0.5 * (1 + math.cos(math.pi * self.current_epoch / self.max_epochs))
        elif self.scheduler_type == 'linear':
            ratio = 1 - self.current_epoch / self.max_epochs
        else:
            ratio = 1.0  # å›ºå®šæƒé‡
        return ratio

    def _get_hard_weights(self, pred):
        # æ··åˆä½¿ç”¨é˜ˆå€¼å’Œè¿žç»­æƒé‡
        with torch.no_grad():
            prob = torch.sigmoid(pred)

            # ç¦»æ•£å›°éš¾åŒºåŸŸæ£€æµ‹
            hard_mask = (prob > self.hard_threshold[0]) & (prob < self.hard_threshold[1])

            # è¿žç»­å›°éš¾åº¦æƒé‡
            hardness = 1 - torch.abs(prob - 0.5) * 2
            cont_weights = hardness ** self.hard_gamma

            # ç»„åˆæƒé‡
            weights = torch.where(hard_mask, cont_weights * self.hard_weight, 1.0)

        return weights

    def forward(self, pred, target,lweight):
        pred = pred.squeeze(1)  # [B,1,H,W] â†’ [B,H,W]
        target = target.float()
        # èŽ·å–æ¸è¿›å¼æƒé‡
        with torch.no_grad():  # ðŸŸ¢ å…³é—­æ¢¯åº¦è®¡ç®—
            ratio = self._get_weight_ratio()
            hard_weights = self._get_hard_weights(pred.detach())  # ðŸŸ¢ åˆ†ç¦»è®¡ç®—å›¾
            hard_weights = hard_weights / (hard_weights.mean() + 1e-6) * 0.5 + 0.5
        #ratio = self._get_weight_ratio()
        # èŽ·å–å›°éš¾æ ·æœ¬æƒé‡
        #hard_weights = self._get_hard_weights(pred)
        # å‡å€¼å½’ä¸€åŒ– (å…³é”®æ­¥éª¤)
        # æ·»åŠ å¹³æ»‘ç³»æ•°
        #hard_weights = hard_weights / (hard_weights.mean() + 1e-6) * 0.5 + 0.5

        # åŠ æƒæŸå¤±è®¡ç®—
        rank_loss = (self.rank_loss(pred, target) * hard_weights).mean()
        focal_loss = (self.focal_loss(pred, target) * hard_weights).mean()

        return (ratio * rank_loss + (1 - ratio) * focal_loss) * lweight
